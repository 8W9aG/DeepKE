<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>deepke.name_entity_re.few_shot.models.modeling_bart &mdash; DeepKE 0.2.97 documentation</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/clipboard.min.js"></script>
        <script src="../../../../../_static/copybutton.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../index.html" class="icon icon-home"> DeepKE
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../start.html">Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../install.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepke.html">DeepKE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">DeepKE</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../../index.html">Module code</a> &raquo;</li>
      <li>deepke.name_entity_re.few_shot.models.modeling_bart</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for deepke.name_entity_re.few_shot.models.modeling_bart</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;PyTorch BART model, ported from the fairseq repo.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>

<span class="kn">from</span> <span class="nn">transformers.modeling_bart</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">_CONFIG_FOR_DOC</span> <span class="o">=</span> <span class="s2">&quot;BartConfig&quot;</span>
<span class="n">_TOKENIZER_FOR_DOC</span> <span class="o">=</span> <span class="s2">&quot;BartTokenizer&quot;</span>

<span class="n">BART_PRETRAINED_MODEL_ARCHIVE_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;facebook/bart-base&quot;</span><span class="p">,</span>
    <span class="s2">&quot;facebook/bart-large&quot;</span><span class="p">,</span>
    <span class="s2">&quot;facebook/bart-large-mnli&quot;</span><span class="p">,</span>
    <span class="s2">&quot;facebook/bart-large-cnn&quot;</span><span class="p">,</span>
    <span class="s2">&quot;facebook/bart-large-xsum&quot;</span><span class="p">,</span>
    <span class="s2">&quot;facebook/mbart-large-en-ro&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="c1"># This list is incomplete. See all BART models at https://huggingface.co/models?filter=bart</span>


<span class="n">BART_START_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic</span>
<span class="s2">    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,</span>
<span class="s2">    pruning heads etc.)</span>

<span class="s2">    This model is also a PyTorch `torch.nn.Module &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&gt;`__ subclass.</span>
<span class="s2">    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general</span>
<span class="s2">    usage and behavior.</span>

<span class="s2">    Parameters:</span>
<span class="s2">        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.</span>
<span class="s2">            Initializing with a config file does not load the weights associated with the model, only the configuration.</span>
<span class="s2">            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.</span>

<span class="s2">&quot;&quot;&quot;</span>

<span class="n">BART_GENERATION_EXAMPLE</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Summarization example::</span>

<span class="s2">        &gt;&gt;&gt; from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig</span>

<span class="s2">        &gt;&gt;&gt; # see ``examples/summarization/bart/run_eval.py`` for a longer example</span>
<span class="s2">        &gt;&gt;&gt; model = BartForConditionalGeneration.from_pretrained(&#39;facebook/bart-large-cnn&#39;)</span>
<span class="s2">        &gt;&gt;&gt; tokenizer = BartTokenizer.from_pretrained(&#39;facebook/bart-large-cnn&#39;)</span>

<span class="s2">        &gt;&gt;&gt; ARTICLE_TO_SUMMARIZE = &quot;My friends are cool but they eat too many carbs.&quot;</span>
<span class="s2">        &gt;&gt;&gt; inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=&#39;pt&#39;)</span>

<span class="s2">        &gt;&gt;&gt; # Generate Summary</span>
<span class="s2">        &gt;&gt;&gt; summary_ids = model.generate(inputs[&#39;input_ids&#39;], num_beams=4, max_length=5, early_stopping=True)</span>
<span class="s2">        &gt;&gt;&gt; print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])</span>

<span class="s2">&quot;&quot;&quot;</span>

<span class="n">BART_INPUTS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Args:</span>
<span class="s2">        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):</span>
<span class="s2">            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide</span>
<span class="s2">            it.</span>

<span class="s2">            Indices can be obtained using :class:`~transformers.BartTokenizer`.</span>
<span class="s2">            See :meth:`transformers.PreTrainedTokenizer.encode` and</span>
<span class="s2">            :meth:`transformers.PreTrainedTokenizer.__call__` for details.</span>

<span class="s2">            `What are input IDs? &lt;../glossary.html#input-ids&gt;`__</span>
<span class="s2">        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="s2">            Mask to avoid performing attention on padding token indices.</span>
<span class="s2">            Mask values selected in ``[0, 1]``:</span>

<span class="s2">            - 1 for tokens that are **not masked**,</span>
<span class="s2">            - 0 for tokens that are **masked**.</span>

<span class="s2">            `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="s2">        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):</span>
<span class="s2">            Provide for translation and summarization training. By default, the model will create this tensor by</span>
<span class="s2">            shifting the :obj:`input_ids` to the right, following the paper.</span>
<span class="s2">        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`):</span>
<span class="s2">            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will</span>
<span class="s2">            also be used by default.</span>

<span class="s2">            If you want to change padding behavior, you should read :func:`modeling_bart._prepare_decoder_inputs` and</span>
<span class="s2">            modify to your needs. See diagram 1 in `the paper &lt;https://arxiv.org/abs/1910.13461&gt;`__ for more</span>
<span class="s2">            information on the default strategy.</span>
<span class="s2">        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`):</span>
<span class="s2">            Tuple consists of (:obj:`last_hidden_state`, `optional`: :obj:`hidden_states`, `optional`: :obj:`attentions`)</span>
<span class="s2">            :obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) is a</span>
<span class="s2">            sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of</span>
<span class="s2">            the decoder.</span>
<span class="s2">        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):</span>
<span class="s2">            Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</span>

<span class="s2">            If :obj:`past_key_values` are used, the user can optionally input only the last</span>
<span class="s2">            ``decoder_input_ids`` (those that don&#39;t have their past key value states given to this model) of shape</span>
<span class="s2">            :obj:`(batch_size, 1)` instead of all ``decoder_input_ids`` of shape :obj:`(batch_size, sequence_length)`.</span>
<span class="s2">        use_cache (:obj:`bool`, `optional`):</span>
<span class="s2">            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up</span>
<span class="s2">            decoding (see :obj:`past_key_values`).</span>
<span class="s2">        output_attentions (:obj:`bool`, `optional`):</span>
<span class="s2">            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned</span>
<span class="s2">            tensors for more detail.</span>
<span class="s2">        output_hidden_states (:obj:`bool`, `optional`):</span>
<span class="s2">            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for</span>
<span class="s2">            more detail.</span>
<span class="s2">        return_dict (:obj:`bool`, `optional`):</span>
<span class="s2">            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>
<span class="s2">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="invert_mask"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.invert_mask">[docs]</a><span class="k">def</span> <span class="nf">invert_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Turns 1-&gt;0, 0-&gt;1, False-&gt;True, True-&gt; False&quot;&quot;&quot;</span>
    <span class="c1"># print(&quot;attention_mask.shape: &quot;, attention_mask.shape)</span>
    <span class="k">assert</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_prepare_bart_decoder_inputs</span><span class="p">(</span>
        <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">decoder_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">causal_mask_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if</span>
<span class="sd">    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.</span>
<span class="sd">    Note: this is not called during generation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">)</span>
    <span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">decoder_padding_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">decoder_padding_mask</span> <span class="o">=</span> <span class="n">make_padding_mask</span><span class="p">(</span><span class="n">decoder_input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">decoder_padding_mask</span> <span class="o">=</span> <span class="n">invert_mask</span><span class="p">(</span><span class="n">decoder_padding_mask</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">decoder_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_padding_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># never mask leading token, even if it is pad</span>
        <span class="n">decoder_padding_mask</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoder_padding_mask</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">fill_with_neg_inf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">))</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">tmp</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">tmp</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">mask</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tmp</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">causal_mask_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">decoder_input_ids</span><span class="p">,</span> <span class="n">decoder_padding_mask</span><span class="p">,</span> <span class="n">causal_mask</span>


<div class="viewcode-block" id="PretrainedBartModel"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.PretrainedBartModel">[docs]</a><span class="k">class</span> <span class="nc">PretrainedBartModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">BartConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="n">std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">init_std</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">SinusoidalPositionalEmbedding</span><span class="p">):</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">module</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dummy_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">pad_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_token</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dummy_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token</span><span class="p">),</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">dummy_inputs</span></div>


<span class="k">def</span> <span class="nf">_make_linear_from_emb</span><span class="p">(</span><span class="n">emb</span><span class="p">):</span>
    <span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">lin_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">lin_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
    <span class="k">return</span> <span class="n">lin_layer</span>


<span class="c1"># Helper Functions, mostly for making masks</span>
<span class="k">def</span> <span class="nf">_check_shapes</span><span class="p">(</span><span class="n">shape_1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">shape_1</span> <span class="o">!=</span> <span class="n">shape2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;shape mismatch: </span><span class="si">{}</span><span class="s2"> != </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">shape_1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">))</span>


<div class="viewcode-block" id="shift_tokens_right"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.shift_tokens_right">[docs]</a><span class="k">def</span> <span class="nf">shift_tokens_right</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Shift input ids one token to the right, and wrap the last non pad token (usually &lt;eos&gt;).&quot;&quot;&quot;</span>
    <span class="n">prev_output_tokens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">index_of_eos</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">prev_output_tokens</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index_of_eos</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">prev_output_tokens</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">prev_output_tokens</span></div>


<div class="viewcode-block" id="make_padding_mask"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.make_padding_mask">[docs]</a><span class="k">def</span> <span class="nf">make_padding_mask</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True for pad tokens&quot;&quot;&quot;</span>
    <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">padding_idx</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">padding_mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">padding_mask</span></div>


<span class="c1"># Helper Modules</span>


<div class="viewcode-block" id="EncoderLayer"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.EncoderLayer">[docs]</a><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">BartConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span>
        <span class="c1"># self.self_attn = Attention(self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout)</span>
        <span class="c1"># Lilei:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_attention_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
                                    <span class="n">cache_key</span><span class="o">=</span><span class="s1">&#39;encoder&#39;</span><span class="p">,</span> <span class="n">use_prompt</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_prompt</span><span class="p">,</span> <span class="n">preseqlen</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">preseqlen</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">normalize_before</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">activation_function</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_ffn_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_ffn_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1"># Lilei:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preseqlen</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">preseqlen</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_prompt</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_prompt</span>

    <span class="c1"># def forward(self, x, encoder_padding_mask, output_attentions=False):</span>
    <span class="c1"># Lilei:</span>
<div class="viewcode-block" id="EncoderLayer.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.EncoderLayer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_padding_mask</span><span class="p">,</span> <span class="n">layer_state</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`</span>
<span class="sd">            encoder_padding_mask (ByteTensor): binary ByteTensor of shape</span>
<span class="sd">                `(batch, src_len)` where padding elements are indicated by ``1``.</span>
<span class="sd">            for t_tgt, t_src is excluded (or masked out), =0 means it is</span>
<span class="sd">            included in attention</span>

<span class="sd">        Returns:</span>
<span class="sd">            encoded output of shape `(seq_len, batch, embed_dim)`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="c1"># query=x, key=x, key_padding_mask=encoder_padding_mask, output_attentions=output_attentions</span>
            <span class="c1"># Lilei:</span>
            <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> <span class="n">query</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">encoder_padding_mask</span><span class="p">,</span> <span class="n">layer_state</span><span class="o">=</span><span class="n">layer_state</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Lilei:</span>
        <span class="c1"># if torch.isinf(x).any() or torch.isnan(x).any():</span>
        <span class="c1">#     clamp_value = torch.finfo(x.dtype).max - 1000</span>
        <span class="c1">#     x = torch.clamp(x, min=-clamp_value, max=clamp_value)</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_weights</span></div></div>


<div class="viewcode-block" id="BartEncoder"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartEncoder">[docs]</a><span class="k">class</span> <span class="nc">BartEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer</span>
<span class="sd">    is a :class:`EncoderLayer`.</span>

<span class="sd">    Args:</span>
<span class="sd">        config: BartConfig</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">BartConfig</span><span class="p">,</span> <span class="n">embed_tokens</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">encoder_layerdrop</span>

        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_source_positions</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">embed_tokens</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">static_position_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">SinusoidalPositionalEmbedding</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">LearnedPositionalEmbedding</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span>
                <span class="n">embed_dim</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
                <span class="n">config</span><span class="o">.</span><span class="n">extra_pos_embeddings</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_embedding</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">normalize_embedding</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="c1"># mbart has one extra layer_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">add_final_layer_norm</span> <span class="k">else</span> <span class="kc">None</span>

<div class="viewcode-block" id="BartEncoder.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartEncoder.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="c1"># self, input_ids, attention_mask=None, output_attentions=False, output_hidden_states=False, return_dict=False</span>
            <span class="c1"># Lilei:</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (LongTensor): tokens in the source language of shape</span>
<span class="sd">                `(batch, src_len)`</span>
<span class="sd">            attention_mask (torch.LongTensor): indicating which indices are padding tokens.</span>
<span class="sd">        Returns:</span>
<span class="sd">            BaseModelOutput or Tuple comprised of:</span>
<span class="sd">                - **x** (Tensor): the last encoder layer&#39;s output of</span>
<span class="sd">                  shape `(src_len, batch, embed_dim)`</span>
<span class="sd">                - **encoder_states** (tuple(torch.FloatTensor)): all intermediate</span>
<span class="sd">                  hidden states of shape `(src_len, batch, embed_dim)`.</span>
<span class="sd">                  Only populated if *output_hidden_states:* is True.</span>
<span class="sd">                - **all_attentions** (tuple(torch.FloatTensor)): Attention weights for each layer.</span>
<span class="sd">                During training might not be of length n_layers because of layer dropout.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># check attention mask and invert</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># print(&quot;encoder attention_mask.shape: &quot;, attention_mask.shape)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">invert_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span>
        <span class="n">embed_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">embed_pos</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># B x T x C -&gt; T x B x C</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="c1"># LiLei</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">encoder_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">):</span>  <span class="c1"># skip the layer</span>
                <span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_state</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">layer_state</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">all_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">attn</span><span class="p">,)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">encoder_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># T x B x C -&gt; B x T x C</span>
            <span class="n">encoder_states</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">hidden_state</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">hidden_state</span> <span class="ow">in</span> <span class="n">encoder_states</span><span class="p">)</span>

        <span class="c1"># T x B x C -&gt; B x T x C</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">,</span> <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">)</span></div>

    
    <span class="c1"># LiLei: append</span>
<div class="viewcode-block" id="BartEncoder.forward_with_encoder_past"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartEncoder.forward_with_encoder_past">[docs]</a>    <span class="k">def</span> <span class="nf">forward_with_encoder_past</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (LongTensor): tokens in the source language of shape</span>
<span class="sd">                `(batch, src_len)`</span>
<span class="sd">            attention_mask (torch.LongTensor): indicating which indices are padding tokens.</span>
<span class="sd">        Returns:</span>
<span class="sd">            BaseModelOutput or Tuple comprised of:</span>
<span class="sd">                - **x** (Tensor): the last encoder layer&#39;s output of</span>
<span class="sd">                  shape `(src_len, batch, embed_dim)`</span>
<span class="sd">                - **encoder_states** (tuple(torch.FloatTensor)): all intermediate</span>
<span class="sd">                  hidden states of shape `(src_len, batch, embed_dim)`.</span>
<span class="sd">                  Only populated if *output_hidden_states:* is True.</span>
<span class="sd">                - **all_attentions** (tuple(torch.FloatTensor)): Attention weights for each layer.</span>
<span class="sd">                During training might not be of length n_layers because of layer dropout.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># check attention mask and invert</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">invert_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span>
        <span class="n">embed_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">embed_pos</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># B x T x C -&gt; T x B x C</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">encoder_cache</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">encoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">encoder_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">):</span>  <span class="c1"># skip the layer</span>
                <span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_state</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">encoder_layer</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">layer_state</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">)</span>
                <span class="c1"># print(&#39;new layer state&#39;, layer_state)</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">all_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">attn</span><span class="p">,)</span>

            <span class="c1"># URGENT</span>
            <span class="n">encoder_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_state</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">encoder_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># T x B x C -&gt; B x T x C</span>
            <span class="n">encoder_states</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">hidden_state</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">hidden_state</span> <span class="ow">in</span> <span class="n">encoder_states</span><span class="p">)</span>

        <span class="c1"># T x B x C -&gt; B x T x C</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">,</span>
                               <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">encoder_cache</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DecoderLayer"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.DecoderLayer">[docs]</a><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">BartConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="c1"># LiLEi:</span>
            <span class="n">cache_key</span><span class="o">=</span><span class="s1">&#39;self&#39;</span><span class="p">,</span>
            <span class="n">use_prompt</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_prompt</span><span class="p">,</span>
            <span class="n">preseqlen</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">preseqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">activation_function</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">normalize_before</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="c1"># LiLei:</span>
            <span class="n">cache_key</span><span class="o">=</span><span class="s1">&#39;encoder_decoder&#39;</span><span class="p">,</span>
            <span class="n">encoder_decoder_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_prompt</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_prompt</span><span class="p">,</span>
            <span class="n">preseqlen</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">preseqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attn_layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_ffn_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

<div class="viewcode-block" id="DecoderLayer.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.DecoderLayer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">idx</span><span class="p">,</span> <span class="c1">###</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">layer_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

        <span class="k">if</span> <span class="n">layer_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">layer_state</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Self Attention</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">self_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span>    <span class="c1">###</span>
            <span class="n">query</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">layer_state</span><span class="o">=</span><span class="n">layer_state</span><span class="p">,</span>  <span class="c1"># adds keys to layer state</span>
            <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">decoder_padding_mask</span><span class="p">,</span>
            <span class="n">attn_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Cross attention</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attn</span><span class="o">.</span><span class="n">cache_key</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="o">.</span><span class="n">cache_key</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attn_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attn</span><span class="p">(</span>
            <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> <span class="c1">###</span>
            <span class="n">query</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">encoder_attn_mask</span><span class="p">,</span>
            <span class="n">layer_state</span><span class="o">=</span><span class="n">layer_state</span><span class="p">,</span>  <span class="c1"># mutates layer state</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attn_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Fully Connected</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_before</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">self_attn_weights</span><span class="p">,</span>
            <span class="n">layer_state</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># just self_attn weights for now, following t5, layer_state = cache for decoding</span></div></div>


<div class="viewcode-block" id="BartDecoder"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartDecoder">[docs]</a><span class="k">class</span> <span class="nc">BartDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer decoder consisting of *config.decoder_layers* layers. Each layer</span>
<span class="sd">    is a :class:`DecoderLayer`.</span>
<span class="sd">    Args:</span>
<span class="sd">        config: BartConfig</span>
<span class="sd">        embed_tokens (torch.nn.Embedding): output embedding</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">BartConfig</span><span class="p">,</span> <span class="n">embed_tokens</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">decoder_layerdrop</span>
        <span class="c1"># self.do_blenderbot_90_layernorm = config.do_blenderbot_90_layernorm  # layernorm variant</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">embed_tokens</span><span class="o">.</span><span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_target_positions</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_embedding</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">embed_tokens</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">static_position_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">SinusoidalPositionalEmbedding</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">LearnedPositionalEmbedding</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span>
                <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
                <span class="n">config</span><span class="o">.</span><span class="n">extra_pos_embeddings</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">)]</span>
        <span class="p">)</span>  <span class="c1"># type: List[DecoderLayer]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_embedding</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">normalize_embedding</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">add_final_layer_norm</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="c1"># self.config = config</span>

<div class="viewcode-block" id="BartDecoder.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartDecoder.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_padding_mask</span><span class="p">,</span>
            <span class="n">decoder_padding_mask</span><span class="p">,</span>
            <span class="n">decoder_causal_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="c1"># LILEI:</span>
            <span class="n">use_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">unused</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Includes several features from &quot;Jointly Learning to Align and</span>
<span class="sd">        Translate with Transformer Models&quot; (Garg et al., EMNLP 2019).</span>

<span class="sd">        Args:</span>
<span class="sd">            input_ids (LongTensor): previous decoder outputs of shape</span>
<span class="sd">                `(batch, tgt_len)`, for teacher forcing</span>
<span class="sd">            encoder_hidden_states: output from the encoder, used for</span>
<span class="sd">                encoder-side attention</span>
<span class="sd">            encoder_padding_mask: for ignoring pad tokens</span>
<span class="sd">            past_key_values (dict or None): dictionary used for storing state during generation</span>

<span class="sd">        Returns:</span>
<span class="sd">            BaseModelOutputWithPast or tuple:</span>
<span class="sd">                - the decoder&#39;s features of shape `(batch, tgt_len, embed_dim)`</span>
<span class="sd">                - the cache</span>
<span class="sd">                - hidden states</span>
<span class="sd">                - attentions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;decoder_cached_states&quot;</span> <span class="ow">in</span> <span class="n">unused</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">unused</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_cached_states&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;decoder_past_key_values&quot;</span> <span class="ow">in</span> <span class="n">unused</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">unused</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_past_key_values&quot;</span><span class="p">)</span>

        <span class="c1"># check attention mask and invert</span>
        <span class="k">if</span> <span class="n">encoder_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_padding_mask</span> <span class="o">=</span> <span class="n">invert_mask</span><span class="p">(</span><span class="n">encoder_padding_mask</span><span class="p">)</span>

        <span class="c1"># embed positions</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">positions</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_scale</span>
        <span class="c1"># LILEI: </span>
        <span class="c1"># if self.do_blenderbot_90_layernorm:</span>
        <span class="c1">#     x = self.layernorm_embedding(x)</span>
        <span class="c1">#     x += positions</span>
        <span class="c1"># else:</span>
        <span class="c1">#     x += positions</span>
        <span class="c1">#     x = self.layernorm_embedding(x)</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">positions</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># Convert to Bart output format: (seq_len, BS, model_dim) -&gt; (BS, seq_len, model_dim)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># decoder layers</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">next_decoder_cache</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="c1"># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="p">,)</span>
            <span class="n">dropout_probability</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dropout_probability</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerdrop</span><span class="p">):</span>
                <span class="k">continue</span>

            <span class="n">layer_state</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

            <span class="n">x</span><span class="p">,</span> <span class="n">layer_self_attn</span><span class="p">,</span> <span class="n">layer_past</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
                <span class="n">idx</span><span class="p">,</span> <span class="c1">###</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attn_mask</span><span class="o">=</span><span class="n">encoder_padding_mask</span><span class="p">,</span>
                <span class="n">decoder_padding_mask</span><span class="o">=</span><span class="n">decoder_padding_mask</span><span class="p">,</span>
                <span class="n">layer_state</span><span class="o">=</span><span class="n">layer_state</span><span class="p">,</span>
                <span class="n">causal_mask</span><span class="o">=</span><span class="n">decoder_causal_mask</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">next_decoder_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_past</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_self_attn</span><span class="p">,)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">:</span>  <span class="c1"># if config.add_final_layer_norm (mBART)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Convert to standard output format: (seq_len, BS, model_dim) -&gt; (BS, seq_len, model_dim)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">hidden_state</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">hidden_state</span> <span class="ow">in</span> <span class="n">all_hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">next_cache</span> <span class="o">=</span> <span class="n">next_decoder_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">next_cache</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_cache</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span>
        <span class="p">)</span></div></div>


<span class="k">def</span> <span class="nf">_reorder_buffer</span><span class="p">(</span><span class="n">attn_cache</span><span class="p">,</span> <span class="n">new_order</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">input_buffer_k</span> <span class="ow">in</span> <span class="n">attn_cache</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">input_buffer_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_cache</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_buffer_k</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">new_order</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attn_cache</span>


<div class="viewcode-block" id="Attention"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.Attention">[docs]</a><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multi-headed attention from &#39;Attention Is All You Need&#39; paper&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">encoder_decoder_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># otherwise self_attention</span>
            <span class="c1"># LILEI</span>
            <span class="n">cache_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">preseqlen</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">use_prompt</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="s2">&quot;embed_dim must be divisible by num_heads&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_decoder_attention</span> <span class="o">=</span> <span class="n">encoder_decoder_attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># self.cache_key = &quot;encoder_decoder&quot; if self.encoder_decoder_attention else &quot;self&quot;</span>
        <span class="c1"># LILEI</span>
        <span class="k">assert</span> <span class="n">cache_key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;self&#39;</span><span class="p">,</span> <span class="s1">&#39;encoder_decoder&#39;</span><span class="p">,</span> <span class="s1">&#39;encoder&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_decoder_attention</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">cache_key</span> <span class="o">==</span> <span class="s1">&#39;encoder_decoder&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_key</span> <span class="o">=</span> <span class="n">cache_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_prompt</span><span class="o">=</span><span class="n">use_prompt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preseqlen</span> <span class="o">=</span> <span class="n">preseqlen</span>

    <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="Attention.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.Attention.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">idx</span><span class="p">,</span> <span class="c1">###</span>
            <span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
            <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">layer_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">attn_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Input shape: Time(SeqLen) x Batch x Channel&quot;&quot;&quot;</span>
        <span class="n">static_kv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_decoder_attention</span>
        <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span>
        <span class="k">assert</span> <span class="nb">list</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">==</span> <span class="p">[</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">]</span>
        <span class="c1"># get here for encoder decoder cause of static_kv</span>
        <span class="c1"># LILEI</span>
        <span class="n">no_extend</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">layer_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># reuse k,v and encoder_padding_mask</span>
            <span class="n">saved_state</span> <span class="o">=</span> <span class="n">layer_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_key</span><span class="p">,</span> <span class="p">{})</span>
            <span class="c1"># if &quot;prev_key&quot; in saved_state and static_kv:</span>
            <span class="c1">#     # previous time steps are cached - no need to recompute key and value if they are static</span>
            <span class="c1">#     key = None</span>
            <span class="c1"># LILEI</span>
            <span class="n">use_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_prompt</span>
            <span class="n">preseqlen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preseqlen</span>
            <span class="k">if</span> <span class="s2">&quot;prev_key&quot;</span> <span class="ow">in</span> <span class="n">saved_state</span> <span class="ow">and</span> <span class="n">static_kv</span> <span class="ow">and</span> <span class="n">use_prompt</span><span class="p">:</span> <span class="c1"># generation time AND compute cross attention.</span>
                <span class="n">computed_len</span> <span class="o">=</span> <span class="n">saved_state</span><span class="p">[</span><span class="s1">&#39;prev_key&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                <span class="c1"># print(computed_len)</span>
                <span class="k">if</span> <span class="n">computed_len</span> <span class="o">&gt;</span> <span class="n">preseqlen</span><span class="p">:</span>
                    <span class="c1"># print(&#39;Happen for generation, NOT for training.&#39;)</span>
                    <span class="n">key</span><span class="o">=</span><span class="kc">None</span>
                    <span class="n">no_extend</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">elif</span> <span class="s2">&quot;prev_key&quot;</span> <span class="ow">in</span> <span class="n">saved_state</span> <span class="ow">and</span> <span class="n">static_kv</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">use_prompt</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">no_extend</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">saved_state</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">layer_state</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span>
        <span class="k">if</span> <span class="n">static_kv</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">v</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">saved_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># k, v, key_padding_mask = self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_saved_state</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">saved_state</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">no_extend</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
        <span class="c1"># # Update cache</span>
        <span class="c1"># layer_state[self.cache_key] = {</span>
        <span class="c1">#     &quot;prev_key&quot;: k.view(bsz, self.num_heads, -1, self.head_dim),</span>
        <span class="c1">#     &quot;prev_value&quot;: v.view(bsz, self.num_heads, -1, self.head_dim),</span>
        <span class="c1">#     &quot;prev_key_padding_mask&quot;: key_padding_mask if not static_kv else None,</span>
        <span class="c1"># }</span>
        <span class="c1"># LILEI</span>
        <span class="c1"># Update cache</span>
        <span class="n">layer_state</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;prev_key&quot;</span><span class="p">:</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">),</span>
            <span class="s2">&quot;prev_value&quot;</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">),</span>
            <span class="c1"># &quot;prev_key_padding_mask&quot;: key_padding_mask if not static_kv else None,</span>
            <span class="s2">&quot;prev_key_padding_mask&quot;</span><span class="p">:</span> <span class="n">key_padding_mask</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">assert</span> <span class="n">k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">src_len</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">assert</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">attn_mask</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>

        <span class="c1"># This is part of a workaround to get around fork/join parallelism not supporting Optional types.</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span>
            <span class="n">bsz</span><span class="p">,</span>
            <span class="n">src_len</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># don&#39;t attend to padding symbols</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
            <span class="n">reshaped</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">reshaped</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">attn_weights</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span></div>

    <span class="k">def</span> <span class="nf">_use_saved_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">saved_state</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">static_kv</span><span class="p">,</span> <span class="n">bsz</span><span class="p">):</span>
        <span class="c1"># saved states are stored with shape (bsz, num_heads, seq_len, head_dim)</span>
        <span class="k">if</span> <span class="s2">&quot;prev_key&quot;</span> <span class="ow">in</span> <span class="n">saved_state</span><span class="p">:</span>
            <span class="n">_prev_key</span> <span class="o">=</span> <span class="n">saved_state</span><span class="p">[</span><span class="s2">&quot;prev_key&quot;</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">_prev_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="c1"># print(_prev_key.shape)</span>
            <span class="c1"># print(bsz, self.num_heads, self.head_dim, bsz* self.num_heads)</span>
            <span class="n">prev_key</span> <span class="o">=</span> <span class="n">_prev_key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">static_kv</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">prev_key</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">prev_key</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;prev_value&quot;</span> <span class="ow">in</span> <span class="n">saved_state</span><span class="p">:</span>
            <span class="n">_prev_value</span> <span class="o">=</span> <span class="n">saved_state</span><span class="p">[</span><span class="s2">&quot;prev_value&quot;</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">_prev_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">prev_value</span> <span class="o">=</span> <span class="n">_prev_value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">static_kv</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">prev_value</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">prev_value</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">prev_key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;prev_key_padding_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prev_key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># print(&#39;using this prev key_padding&#39;,prev_key_padding_mask.shape)</span>
            <span class="k">if</span> <span class="n">static_kv</span><span class="p">:</span>
                <span class="n">new_key_padding_mask</span> <span class="o">=</span> <span class="n">prev_key_padding_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># print(key_padding_mask.shape if key_padding_mask is not None else None )</span>
                <span class="c1"># print(prev_key_padding_mask.shape if prev_key_padding_mask is not None else None)</span>
                <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">new_key_padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">prev_key_padding_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_key_padding_mask</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_key_padding_mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span>
        <span class="k">return</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">new_key_padding_mask</span></div>


<div class="viewcode-block" id="BartClassificationHead"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartClassificationHead">[docs]</a><span class="k">class</span> <span class="nc">BartClassificationHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Head for sentence-level classification tasks.&quot;&quot;&quot;</span>

    <span class="c1"># This can trivially be shared with RobertaClassificationHead</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="p">,</span>
            <span class="n">inner_dim</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">,</span>
            <span class="n">pooler_dropout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">pooler_dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<div class="viewcode-block" id="BartClassificationHead.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartClassificationHead.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="LearnedPositionalEmbedding"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.LearnedPositionalEmbedding">[docs]</a><span class="k">class</span> <span class="nc">LearnedPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This module learns positional embeddings up to a fixed maximum size.</span>
<span class="sd">    Padding ids are ignored by either offsetting based on padding_idx</span>
<span class="sd">    or by setting padding_idx to None and ensuring that the appropriate</span>
<span class="sd">    position ids are passed to the forward function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
        <span class="c1"># Bart is set up so that if padding_idx is specified then offset the embedding ids by 2</span>
        <span class="c1"># and adjust num_embeddings appropriately. Other models dont have this hack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offset</span> <span class="o">=</span> <span class="n">offset</span>
        <span class="k">assert</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">num_embeddings</span> <span class="o">+=</span> <span class="n">offset</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">)</span>

<div class="viewcode-block" id="LearnedPositionalEmbedding.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.LearnedPositionalEmbedding.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Input is expected to be of size [bsz x seqlen].&quot;&quot;&quot;</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">positions</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># called before slicing</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># starts at 0, ends at 1-seq_len</span>
            <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">positions</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LayerNorm"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.LayerNorm">[docs]</a><span class="k">def</span> <span class="nf">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">apex.normalization</span> <span class="kn">import</span> <span class="n">FusedLayerNorm</span>

            <span class="k">return</span> <span class="n">FusedLayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="p">)</span></div>


<div class="viewcode-block" id="fill_with_neg_inf"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.fill_with_neg_inf">[docs]</a><span class="k">def</span> <span class="nf">fill_with_neg_inf</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;FP16-compatible function that fills a input_ids with -inf.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">t</span><span class="p">)</span></div>


<span class="c1"># Public API</span>
<span class="k">def</span> <span class="nf">_get_shape</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<div class="viewcode-block" id="BartModel"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartModel">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="s2">&quot;The bare BART Model outputting raw hidden-states without any specific head on top.&quot;</span><span class="p">,</span>
    <span class="n">BART_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">BartModel</span><span class="p">(</span><span class="n">PretrainedBartModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">BartConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="n">padding_idx</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">BartEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">BartDecoder</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

<div class="viewcode-block" id="BartModel.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartModel.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">BART_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;facebook/bart-large&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">Seq2SeqModelOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="c1"># LILEI</span>
            <span class="n">use_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;decoder_past_key_values&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_past_key_values&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="c1"># # make masks if user doesn&#39;t supply</span>
        <span class="c1"># if not use_cache:</span>
        <span class="c1">#     decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(</span>
        <span class="c1">#         self.config,</span>
        <span class="c1">#         input_ids,</span>
        <span class="c1">#         decoder_input_ids=decoder_input_ids,</span>
        <span class="c1">#         decoder_padding_mask=decoder_attention_mask,</span>
        <span class="c1">#         causal_mask_dtype=self.shared.weight.dtype,</span>
        <span class="c1">#     )</span>
        <span class="c1"># else:</span>
        <span class="c1">#     decoder_padding_mask, causal_mask = None, None</span>

        <span class="c1"># LILEI causal_mask</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;not use cache&#39;</span><span class="p">)</span>
            <span class="n">decoder_input_ids</span><span class="p">,</span> <span class="n">decoder_padding_mask</span><span class="p">,</span> <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">_prepare_bart_decoder_inputs</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
                <span class="n">decoder_padding_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
                <span class="n">causal_mask_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">use_prompt</span><span class="p">:</span>
                <span class="n">bsz</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;self&#39;</span><span class="p">][</span><span class="s1">&#39;prev_value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                <span class="n">tgt_len</span> <span class="o">=</span> <span class="n">decoder_input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">temp_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">causal_mask</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="c1">#tgtlen, preseqlen</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">temp_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#tgtlen, preseqlen+tgtlen</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;USE cache&#39;</span><span class="p">)</span>
            <span class="n">decoder_padding_mask</span><span class="p">,</span> <span class="n">causal_mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


        <span class="k">assert</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="c1"># LILEI</span>
                <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOuput when return_dict=False</span>
        <span class="k">elif</span> <span class="n">return_dict</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">):</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
                <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
            <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">decoder_padding_mask</span><span class="p">,</span>
            <span class="n">decoder_causal_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">decoder_outputs</span> <span class="o">+</span> <span class="n">encoder_outputs</span>

        <span class="k">return</span> <span class="n">Seq2SeqModelOutput</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="BartModel.get_input_embeddings"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartModel.get_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span></div>

<div class="viewcode-block" id="BartModel.set_input_embeddings"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartModel.set_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span></div>

<div class="viewcode-block" id="BartModel.get_output_embeddings"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartModel.get_output_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_make_linear_from_emb</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>  <span class="c1"># make it on the fly</span></div></div>


<div class="viewcode-block" id="BartForConditionalGeneration"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForConditionalGeneration">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="s2">&quot;The BART Model with a language modeling head. Can be used for summarization.&quot;</span><span class="p">,</span> <span class="n">BART_START_DOCSTRING</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">BartForConditionalGeneration</span><span class="p">(</span><span class="n">PretrainedBartModel</span><span class="p">):</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>
    <span class="n">authorized_missing_keys</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;final_logits_bias&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;encoder\.version&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;decoder\.version&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">BartConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># LILEI</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Init the BartForConditionalGeneration Model with config.use_prompt=</span><span class="si">{}</span><span class="s1">, &#39;</span>
              <span class="s1">&#39;config.preseqlen=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">use_prompt</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">preseqlen</span><span class="p">))</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="n">BartModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">base_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;final_logits_bias&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">)))</span>

<div class="viewcode-block" id="BartForConditionalGeneration.resize_token_embeddings"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForConditionalGeneration.resize_token_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
        <span class="n">old_num_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">num_embeddings</span>
        <span class="n">new_embeddings</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">new_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_resize_final_logits_bias</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">old_num_tokens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_embeddings</span></div>

    <span class="k">def</span> <span class="nf">_resize_final_logits_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">old_num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="o">&lt;=</span> <span class="n">old_num_tokens</span><span class="p">:</span>
            <span class="n">new_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_logits_bias</span><span class="p">[:,</span> <span class="p">:</span><span class="n">new_num_tokens</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">extra_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">new_num_tokens</span> <span class="o">-</span> <span class="n">old_num_tokens</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">final_logits_bias</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">new_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">final_logits_bias</span><span class="p">,</span> <span class="n">extra_bias</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;final_logits_bias&quot;</span><span class="p">,</span> <span class="n">new_bias</span><span class="p">)</span>

<div class="viewcode-block" id="BartForConditionalGeneration.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForConditionalGeneration.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">BART_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@replace_return_docstrings</span><span class="p">(</span><span class="n">output_type</span><span class="o">=</span><span class="n">Seq2SeqLMOutput</span><span class="p">,</span> <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">)</span>
    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">BART_GENERATION_EXAMPLE</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="c1"># LILEI</span>
            <span class="n">use_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="o">**</span><span class="n">unused</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">            Labels for computing the masked language modeling loss.</span>
<span class="sd">            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).</span>
<span class="sd">            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens</span>
<span class="sd">            with labels in ``[0, ..., config.vocab_size]``.</span>

<span class="sd">        Returns:</span>

<span class="sd">        Conditional generation example::</span>

<span class="sd">            &gt;&gt;&gt; # Mask filling only works for bart-large</span>
<span class="sd">            &gt;&gt;&gt; from transformers import BartTokenizer, BartForConditionalGeneration</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = BartTokenizer.from_pretrained(&#39;facebook/bart-large&#39;)</span>
<span class="sd">            &gt;&gt;&gt; TXT = &quot;My friends are &lt;mask&gt; but they eat too many carbs.&quot;</span>

<span class="sd">            &gt;&gt;&gt; model = BartForConditionalGeneration.from_pretrained(&#39;facebook/bart-large&#39;)</span>
<span class="sd">            &gt;&gt;&gt; input_ids = tokenizer([TXT], return_tensors=&#39;pt&#39;)[&#39;input_ids&#39;]</span>
<span class="sd">            &gt;&gt;&gt; logits = model(input_ids).logits</span>

<span class="sd">            &gt;&gt;&gt; masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()</span>
<span class="sd">            &gt;&gt;&gt; probs = logits[0, masked_index].softmax(dim=0)</span>
<span class="sd">            &gt;&gt;&gt; values, predictions = probs.topk(5)</span>

<span class="sd">            &gt;&gt;&gt; tokenizer.decode(predictions).split()</span>
<span class="sd">            &gt;&gt;&gt; # [&#39;good&#39;, &#39;great&#39;, &#39;all&#39;, &#39;really&#39;, &#39;very&#39;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;lm_labels&quot;</span> <span class="ow">in</span> <span class="n">unused</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">unused</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lm_labels&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;decoder_cached_states&quot;</span> <span class="ow">in</span> <span class="n">unused</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">unused</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_cached_states&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;decoder_past_key_values&quot;</span> <span class="ow">in</span> <span class="n">unused</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">unused</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_past_key_values&quot;</span><span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">shift_tokens_right</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">use_prompt</span><span class="o">=</span><span class="n">use_prompt</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">lm_logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">final_logits_bias</span><span class="p">)</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="c1"># TODO(SS): do we need to ignore pad tokens in labels?</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_attentions</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="BartForConditionalGeneration.prepare_inputs_for_generation"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForConditionalGeneration.prepare_inputs_for_generation">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">decoder_input_ids</span><span class="p">,</span> <span class="n">past</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">use_cache</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="c1"># assert False, &#39;check if pass&#39;</span>
        <span class="c1"># print(&#39;HELLO WORLD, this is the generation template used &#39;)</span>
        <span class="k">if</span> <span class="n">past</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># print(&#39;Prepare for Generation: only at the beginnning. &#39;, kwargs[&#39;past_key_values&#39;])</span>
            <span class="k">if</span> <span class="s1">&#39;past_key_values&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;past_key_values&#39;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">past</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># encoder_outputs is defined. input_ids not needed</span>
            <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past</span><span class="p">,</span>
            <span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">:</span> <span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>  <span class="c1"># change this to avoid caching (presumably for debugging)</span>
        <span class="p">}</span></div>

<div class="viewcode-block" id="BartForConditionalGeneration.adjust_logits_during_generation"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForConditionalGeneration.adjust_logits_during_generation">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_logits_during_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">force_bos_token_to_be_generated</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_force_token_ids_generation</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_force_token_ids_generation</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>

    <span class="k">def</span> <span class="nf">_force_token_ids_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">token_id</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float(&quot;inf&quot;))&quot;&quot;&quot;</span>
        <span class="n">scores</span><span class="p">[:,</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">token_id</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past</span><span class="p">:</span>
            <span class="c1"># get the correct batch idx from decoder layer&#39;s batch dim for cross and self-attn</span>
            <span class="n">layer_past_new</span> <span class="o">=</span> <span class="p">{</span>
                <span class="c1"># attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()</span>
                <span class="n">attn_key</span><span class="p">:</span> <span class="n">_reorder_buffer</span><span class="p">(</span><span class="n">attn_cache</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">attn_key</span><span class="p">,</span> <span class="n">attn_cache</span> <span class="ow">in</span> <span class="n">layer_past</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">attn_key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;encoder_decoder&#39;</span><span class="p">,</span> <span class="s1">&#39;self&#39;</span><span class="p">]</span>
            <span class="p">}</span>
            <span class="n">reordered_past</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_past_new</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reordered_past</span>

<div class="viewcode-block" id="BartForConditionalGeneration.get_encoder"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForConditionalGeneration.get_encoder">[docs]</a>    <span class="k">def</span> <span class="nf">get_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span></div>

<div class="viewcode-block" id="BartForConditionalGeneration.get_output_embeddings"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForConditionalGeneration.get_output_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_make_linear_from_emb</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>  <span class="c1"># make it on the fly</span></div></div>


<div class="viewcode-block" id="BartForSequenceClassification"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForSequenceClassification">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks. &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">BART_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">BartForSequenceClassification</span><span class="p">(</span><span class="n">PretrainedBartModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">BartConfig</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">BartModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classification_head</span> <span class="o">=</span> <span class="n">BartClassificationHead</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">classifier_dropout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classification_head</span><span class="o">.</span><span class="n">dense</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classification_head</span><span class="o">.</span><span class="n">out_proj</span><span class="p">)</span>

<div class="viewcode-block" id="BartForSequenceClassification.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForSequenceClassification.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">BART_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;facebook/bart-large&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">Seq2SeqSequenceClassifierOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">            Labels for computing the sequence classification/regression loss.</span>
<span class="sd">            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.</span>
<span class="sd">            If :obj:`config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># last hidden state</span>
        <span class="n">eos_mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">eos_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All examples must have the same number of &lt;eos&gt; tokens.&quot;</span><span class="p">)</span>
        <span class="n">sentence_representation</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">eos_mask</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classification_head</span><span class="p">(</span><span class="n">sentence_representation</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqSequenceClassifierOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_attentions</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="BartForQuestionAnswering"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForQuestionAnswering">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layer on top of</span>
<span class="sd">    the hidden-states output to compute `span start logits` and `span end logits`). &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">BART_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">BartForQuestionAnswering</span><span class="p">(</span><span class="n">PretrainedBartModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">BartModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qa_outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qa_outputs</span><span class="p">)</span>

<div class="viewcode-block" id="BartForQuestionAnswering.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.BartForQuestionAnswering.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_callable</span><span class="p">(</span><span class="n">BART_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;facebook/bart-large&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">Seq2SeqQuestionAnsweringModelOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">start_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">end_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">            Labels for position (index) of the start of the labelled span for computing the token classification loss.</span>
<span class="sd">            Positions are clamped to the length of the sequence (`sequence_length`).</span>
<span class="sd">            Position outside of the sequence are not taken into account for computing the loss.</span>
<span class="sd">        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">            Labels for position (index) of the end of the labelled span for computing the token classification loss.</span>
<span class="sd">            Positions are clamped to the length of the sequence (`sequence_length`).</span>
<span class="sd">            Position outside of the sequence are not taken into account for computing the loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">end_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">decoder_input_ids</span><span class="p">,</span>
            <span class="n">decoder_attention_mask</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qa_outputs</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>
        <span class="n">start_logits</span><span class="p">,</span> <span class="n">end_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">start_logits</span> <span class="o">=</span> <span class="n">start_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">end_logits</span> <span class="o">=</span> <span class="n">end_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">end_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If we are on multi-GPU, split add a dimension</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">start_positions</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">start_positions</span> <span class="o">=</span> <span class="n">start_positions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_positions</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">end_positions</span> <span class="o">=</span> <span class="n">end_positions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># sometimes the start/end positions are outside our model inputs, we ignore these terms</span>
            <span class="n">ignored_index</span> <span class="o">=</span> <span class="n">start_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">start_positions</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ignored_index</span><span class="p">)</span>
            <span class="n">end_positions</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ignored_index</span><span class="p">)</span>

            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">ignored_index</span><span class="p">)</span>
            <span class="n">start_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span>
            <span class="n">end_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">end_logits</span><span class="p">,</span> <span class="n">end_positions</span><span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_loss</span> <span class="o">+</span> <span class="n">end_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
                         <span class="n">start_logits</span><span class="p">,</span>
                         <span class="n">end_logits</span><span class="p">,</span>
                     <span class="p">)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">total_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">total_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">Seq2SeqQuestionAnsweringModelOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">total_loss</span><span class="p">,</span>
            <span class="n">start_logits</span><span class="o">=</span><span class="n">start_logits</span><span class="p">,</span>
            <span class="n">end_logits</span><span class="o">=</span><span class="n">end_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
            <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,</span>
            <span class="n">encoder_last_hidden_state</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_last_hidden_state</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">encoder_attentions</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="SinusoidalPositionalEmbedding"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.SinusoidalPositionalEmbedding">[docs]</a><span class="k">class</span> <span class="nc">SinusoidalPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This module produces sinusoidal positional embeddings of any length.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_positions</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_positions</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;odd embedding_dim </span><span class="si">{</span><span class="n">embedding_dim</span><span class="si">}</span><span class="s2"> not supported&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_init_weight</span><span class="p">(</span><span class="n">out</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Identical to the XLM create_sinusoidal_embeddings except features are not interleaved.</span>
<span class="sd">        The cos features are in the 2nd half of the vector. [dim // 2:]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_pos</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">position_enc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[[</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">j</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_pos</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="n">out</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]))</span>  <span class="c1"># This line breaks for odd n_pos</span>
        <span class="n">out</span><span class="p">[:,</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">out</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">out</span>

<div class="viewcode-block" id="SinusoidalPositionalEmbedding.forward"><a class="viewcode-back" href="../../../../../deepke.name_entity_re.few_shot.models.html#deepke.name_entity_re.few_shot.models.modeling_bart.SinusoidalPositionalEmbedding.forward">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Input is expected to be of size [bsz x seqlen].&quot;&quot;&quot;</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">positions</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># called before slicing</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># starts at 0, ends at 1-seq_len</span>
            <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ZJUNLP.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>